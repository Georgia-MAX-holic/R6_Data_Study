{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled18.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPLMxUO/X9WjNhzD1lKId2+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Georgia-MAX-holic/R6_data_stduy/blob/main/R6_Spark_Study_Handling%20Miss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cF0H5ED4sb4s"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark=SparkSession.builder.appName(\"R6_study\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "metadata": {
        "id": "IzQDMWI3su_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/gdrive/MyDrive/cpding/asdf/레식 데이터.csv'\n",
        "df_spark=spark.read.option(\"header\",\"true\").csv(path, inferSchema=True)"
      ],
      "metadata": {
        "id": "NzsfVvXGuXOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " df_spark.show(6)"
      ],
      "metadata": {
        "id": "dQZQaoYNuiKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_spark= df_spark.drop(\"skillrank\")"
      ],
      "metadata": {
        "id": "uUth9laKxmf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_spark.show(5)"
      ],
      "metadata": {
        "id": "gwKdD9x_xv4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_spark.na.drop().show()\n",
        "#this dataset don't have null value , but this grammer can erase the null value"
      ],
      "metadata": {
        "id": "NzFForMi0Oip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### any==how"
      ],
      "metadata": {
        "id": "Po9i7L-o1QXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_spark.na.drop(how=\"all\").show() # 전부 null 이면 행 삭제\n",
        "df_spark.na.drop(how=\"any\").show() # 하나라도 null 이면 행 삭제"
      ],
      "metadata": {
        "id": "pIwiyVG31Upi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##threshold\n",
        "df_spark.na.drop(how=\"any\",thresh=2).show()# null 값이 아닌것이 2개 이상 있어야함 "
      ],
      "metadata": {
        "id": "A9mDpe7sBQum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_spark.na.drop(how=\"any\",subset=[\"nbkills\"]).show() #subset 은 Row를 삭제할 때, 어느 칼럼에 결측치가 있는 Row를 삭제할지를 결정하는 기준이 된다."
      ],
      "metadata": {
        "id": "C9eVoOYqELVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Filling the missing value \n",
        "df_spark.na.fill(\"Missing Values\").show() # null 값을 Missing Values 로 변경"
      ],
      "metadata": {
        "id": "oP3MBjYbNiKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_spark.na.fill(\"Missing Values\", [\"nbwins\", \"nbpicks\"]).show()  # null값을 Missing Values로 변경할거고 그 대상은 nbwins와 nbpicks"
      ],
      "metadata": {
        "id": "wYgkiGuDORfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Imputer\n",
        "\n",
        "imputer = Imputer(\n",
        "  inputCols=[\"nbwins\",\"nbkills\",\"nbdeaths\",\"nbpicks\"],\n",
        "  outputCols=[\"{}_imputed\".format(c) for c in [\"nbwins\",\"nbkills\",\"nbdeaths\",\"nbpicks\"]]\n",
        ").setStrategy(\"mean\")"
      ],
      "metadata": {
        "id": "ndMdcJoIQL7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "HAcx5PzZTA-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add imputation cols to df \n",
        "imputer.fit(df_spark).transform(df_spark).show() # imputed 값 -> 평균값 ,  column 추가 "
      ],
      "metadata": {
        "id": "5xEgncVcSCI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "--f2xVMoUERC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}